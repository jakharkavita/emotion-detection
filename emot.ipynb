{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: pandas in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.1.4)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.16.1)\n",
      "Requirement already satisfied: keras in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (3.3.3)\n",
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (23.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn) (1.11.4)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (68.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.9.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: rich in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras) (13.3.5)\n",
      "Requirement already satisfied: namex in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from keras) (0.11.0)\n",
      "Requirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich->keras) (2.2.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from rich->keras) (2.15.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.41.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich->keras) (0.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.2.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.3)\n",
      "Using cached opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Users\\\\lenovo\\\\anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy pandas matplotlib seaborn scikit-learn tensorflow keras opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load images and labels\n",
    "def load_data(dataset_path):\n",
    "    categories = ['happy', 'sad', 'surprise', 'neutral', 'fear', 'disgust', 'angry']\n",
    "    data = []\n",
    "    labels = []\n",
    "\n",
    "    for category in categories:\n",
    "        path = os.path.join(dataset_path, category)\n",
    "        class_num = categories.index(category)\n",
    "\n",
    "        for img in os.listdir(path):\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(path, img), cv2.IMREAD_GRAYSCALE)\n",
    "                img_array = cv2.resize(img_array, (48, 48)) # Resize to 48x48 pixels\n",
    "                data.append(img_array)\n",
    "                labels.append(class_num)\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "    return np.array(data), np.array(labels)\n",
    "\n",
    "# Load training data\n",
    "train_data, train_labels = load_data('dataset/train')\n",
    "train_data = train_data.reshape(train_data.shape[0], 48, 48, 1)\n",
    "train_data = train_data / 255.0\n",
    "\n",
    "# Load testing data\n",
    "test_data, test_labels = load_data('dataset/test')\n",
    "test_data = test_data.reshape(test_data.shape[0], 48, 48, 1)\n",
    "test_data = test_data / 255.0\n",
    "\n",
    "# One-hot encode labels\n",
    "train_labels = to_categorical(train_labels, num_classes=7)\n",
    "test_labels = to_categorical(test_labels, num_classes=7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">46</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │           <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │        <span style=\"color: #00af00; text-decoration-color: #00af00\">73,856</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │       <span style=\"color: #00af00; text-decoration-color: #00af00\">524,544</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m46\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │           \u001b[38;5;34m320\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_4 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m32\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │        \u001b[38;5;34m18,496\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_4 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_5 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)     │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │        \u001b[38;5;34m73,856\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ max_pooling2d_5 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_6 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │       \u001b[38;5;34m524,544\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dropout_7 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │         \u001b[38;5;34m1,799\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,015</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m619,015\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">619,015</span> (2.36 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m619,015\u001b[0m (2.36 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 112ms/step - accuracy: 0.2439 - loss: 1.8241 - val_accuracy: 0.3731 - val_loss: 1.6080\n",
      "Epoch 2/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 102ms/step - accuracy: 0.3709 - loss: 1.5972 - val_accuracy: 0.4526 - val_loss: 1.4308\n",
      "Epoch 3/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 95ms/step - accuracy: 0.4436 - loss: 1.4406 - val_accuracy: 0.5000 - val_loss: 1.3330\n",
      "Epoch 4/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 97ms/step - accuracy: 0.4836 - loss: 1.3487 - val_accuracy: 0.5174 - val_loss: 1.2786\n",
      "Epoch 5/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 96ms/step - accuracy: 0.5016 - loss: 1.3082 - val_accuracy: 0.5348 - val_loss: 1.2181\n",
      "Epoch 6/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 97ms/step - accuracy: 0.5147 - loss: 1.2761 - val_accuracy: 0.5365 - val_loss: 1.2042\n",
      "Epoch 7/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 100ms/step - accuracy: 0.5287 - loss: 1.2411 - val_accuracy: 0.5430 - val_loss: 1.1987\n",
      "Epoch 8/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 99ms/step - accuracy: 0.5385 - loss: 1.2147 - val_accuracy: 0.5577 - val_loss: 1.1551\n",
      "Epoch 9/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 100ms/step - accuracy: 0.5441 - loss: 1.1927 - val_accuracy: 0.5628 - val_loss: 1.1490\n",
      "Epoch 10/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 105ms/step - accuracy: 0.5591 - loss: 1.1650 - val_accuracy: 0.5632 - val_loss: 1.1414\n",
      "Epoch 11/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 105ms/step - accuracy: 0.5721 - loss: 1.1419 - val_accuracy: 0.5720 - val_loss: 1.1247\n",
      "Epoch 12/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 106ms/step - accuracy: 0.5743 - loss: 1.1270 - val_accuracy: 0.5723 - val_loss: 1.1222\n",
      "Epoch 13/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 106ms/step - accuracy: 0.5801 - loss: 1.1156 - val_accuracy: 0.5769 - val_loss: 1.1135\n",
      "Epoch 14/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 106ms/step - accuracy: 0.5881 - loss: 1.0885 - val_accuracy: 0.5790 - val_loss: 1.1116\n",
      "Epoch 15/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 98ms/step - accuracy: 0.5939 - loss: 1.0809 - val_accuracy: 0.5871 - val_loss: 1.0926\n",
      "Epoch 16/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.6014 - loss: 1.0497 - val_accuracy: 0.5880 - val_loss: 1.0934\n",
      "Epoch 17/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 92ms/step - accuracy: 0.6063 - loss: 1.0427 - val_accuracy: 0.5896 - val_loss: 1.0926\n",
      "Epoch 18/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 95ms/step - accuracy: 0.6160 - loss: 1.0303 - val_accuracy: 0.5834 - val_loss: 1.0950\n",
      "Epoch 19/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 95ms/step - accuracy: 0.6123 - loss: 1.0245 - val_accuracy: 0.5928 - val_loss: 1.0861\n",
      "Epoch 20/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 94ms/step - accuracy: 0.6189 - loss: 1.0162 - val_accuracy: 0.5965 - val_loss: 1.0747\n",
      "Epoch 21/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 95ms/step - accuracy: 0.6236 - loss: 1.0036 - val_accuracy: 0.5947 - val_loss: 1.0839\n",
      "Epoch 22/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 94ms/step - accuracy: 0.6307 - loss: 0.9907 - val_accuracy: 0.5974 - val_loss: 1.0808\n",
      "Epoch 23/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 97ms/step - accuracy: 0.6360 - loss: 0.9674 - val_accuracy: 0.5993 - val_loss: 1.0755\n",
      "Epoch 24/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 99ms/step - accuracy: 0.6459 - loss: 0.9536 - val_accuracy: 0.5985 - val_loss: 1.0802\n",
      "Epoch 25/25\n",
      "\u001b[1m449/449\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 95ms/step - accuracy: 0.6473 - loss: 0.9458 - val_accuracy: 0.6006 - val_loss: 1.0779\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout\n",
    "\n",
    "model = Sequential([\n",
    "    Conv2D(32, (3, 3), activation='relu', input_shape=(48, 48, 1)),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(64, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Conv2D(128, (3, 3), activation='relu'),\n",
    "    MaxPooling2D(2, 2),\n",
    "    Dropout(0.25),\n",
    "    \n",
    "    Flatten(),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dropout(0.5),\n",
    "    Dense(7, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(train_data, train_labels, epochs=25, validation_data=(test_data, test_labels), batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m225/225\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - accuracy: 0.7084 - loss: 0.8246\n",
      "Test accuracy: 60.06%\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print(f'Test accuracy: {accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "model.save('emotion_detection_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Using cached opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from opencv-python-headless) (1.26.4)\n",
      "Using cached opencv_python_headless-4.10.0.82-cp37-abi3-win_amd64.whl (38.8 MB)\n",
      "Installing collected packages: opencv-python-headless\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'c:\\\\Users\\\\lenovo\\\\anaconda3\\\\Lib\\\\site-packages\\\\cv2\\\\cv2.pyd'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gtts in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (2.5.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: requests<3,>=2.27 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gtts) (2.31.0)\n",
      "Requirement already satisfied: click<8.2,>=7.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from gtts) (8.1.7)\n",
      "Requirement already satisfied: colorama in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from click<8.2,>=7.1->gtts) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (from requests<3,>=2.27->gtts) (2024.2.2)\n"
     ]
    }
   ],
   "source": [
    "%pip install gtts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.9.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "print(cv2.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 213\u001b[0m\n\u001b[0;32m    210\u001b[0m         emotion_detected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[38;5;66;03m# Display the resulting frame\u001b[39;00m\n\u001b[1;32m--> 213\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEmotion Detection\u001b[39m\u001b[38;5;124m'\u001b[39m, frame)\n\u001b[0;32m    215\u001b[0m \u001b[38;5;66;03m# Check for key press to recapture emotion\u001b[39;00m\n\u001b[0;32m    216\u001b[0m key \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mwaitKey(\u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xFF\u001b[39m\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.10.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\highgui\\src\\window.cpp:1301: error: (-2:Unspecified error) The function is not implemented. Rebuild the library with Windows, GTK+ 2.x or Cocoa support. If you are on Ubuntu or Debian, install libgtk2.0-dev and pkg-config, then re-run cmake or configure script in function 'cvShowImage'\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import cv2\n",
    "# import pygame\n",
    "# import random\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from gtts import gTTS\n",
    "# import os\n",
    "\n",
    "# # Load your model\n",
    "# model = load_model('emotion_detection_model.h5')\n",
    "\n",
    "# # Function to predict emotion from an image\n",
    "# def predict_emotion(model, img):\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img = cv2.resize(img, (48, 48))\n",
    "#     img = img.reshape(1, 48, 48, 1) / 255.0\n",
    "#     prediction = model.predict(img)\n",
    "#     emotion = np.argmax(prediction)\n",
    "#     return emotion\n",
    "\n",
    "# # Function to generate and play a personalized message\n",
    "# def play_message(emotion):\n",
    "#     emotion_dict = {\n",
    "#         0: 'Aaj aapka mood bahut accha lag raha hai. Lijiye aapke mood ko aur accha karne ke liye pesh hai ek pyara sa song.',\n",
    "#         1: 'Aaj aap thode udas lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa song.',\n",
    "#         2: 'Aap bahut chakit lag rahe hain! Lijiye aapke mood ko aur accha karne ke liye pesh hai ek pyara sa song.',\n",
    "#         3: 'Aap bilkul shaant lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "#         4: 'Aap thoda dar rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "#         5: 'Aapko kuch ghin aa rahi hai. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "#         6: 'Aap gusse mein lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.'\n",
    "#     }\n",
    "\n",
    "#     message = emotion_dict[emotion]\n",
    "#     tts = gTTS(text=message, lang='hi')\n",
    "#     tts.save(\"message.mp3\")\n",
    "#     pygame.mixer.init()\n",
    "#     pygame.mixer.music.load(\"message.mp3\")\n",
    "#     pygame.mixer.music.play()\n",
    "#     while pygame.mixer.music.get_busy():\n",
    "#         continue\n",
    "\n",
    "# # Function to play a song based on the emotion\n",
    "# def play_song(emotion):\n",
    "#     emotion_dict = {0: 'happy', 1: 'sad', 2: 'surprise', 3: 'neutral', 4: 'fear', 5: 'disgust', 6: 'angry'}\n",
    "#     songs = {\n",
    "#         'happy': ['happy.mpeg', 'happy1.mpeg'],\n",
    "#         'sad': ['sad.mpeg', 'sad1.mpeg'],\n",
    "#         'surprise': ['sad.mpeg', 'happy.mpeg'],\n",
    "#         'neutral': ['netural.mpeg', 'netural1.mpeg'],\n",
    "#         'fear': ['sad1.mpeg', 'happy1.mpeg'],\n",
    "#         'disgust': ['netural.mpeg', 'netural1.mpeg'],\n",
    "#         'angry': ['happy.mpeg', 'happy1.']\n",
    "#     }\n",
    "    \n",
    "#     emotion_label = emotion_dict[emotion]\n",
    "#     song = random.choice(songs[emotion_label])\n",
    "    \n",
    "#     # Play the personalized message\n",
    "#     play_message(emotion)\n",
    "    \n",
    "#     # Play the song\n",
    "#     pygame.mixer.music.load(song)\n",
    "#     pygame.mixer.music.play()\n",
    "\n",
    "# # Start video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "#     if not ret:\n",
    "#         break\n",
    "    \n",
    "#     # Define the region of interest for face detection\n",
    "#     face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "#     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "#     faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "    \n",
    "#     for (x, y, w, h) in faces:\n",
    "#         face = frame[y:y+h, x:x+w]\n",
    "#         emotion = predict_emotion(model, face)\n",
    "        \n",
    "#         # Draw a rectangle around the face and label it with the emotion\n",
    "#         cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "#         emotion_dict = {0: 'Happy', 1: 'Sad', 2: 'Surprise', 3: 'Neutral', 4: 'Fear', 5: 'Disgust', 6: 'Angry'}\n",
    "#         emotion_text = emotion_dict[emotion]\n",
    "#         cv2.putText(frame, emotion_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "        \n",
    "        # Play the song for the detected emotion\n",
    "#         play_song(emotion)\n",
    "    \n",
    "#     # Display the resulting frame\n",
    "#     cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "#     # Break the loop on 'q' key press\n",
    "#     if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all OpenCV windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Remove the temporary message file\n",
    "# if os.path.exists(\"message.mp3\"):\n",
    "#     os.remove(\"message.mp3\")\n",
    "# hello hellow hellow \n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# import pygame\n",
    "# import random\n",
    "# from tensorflow.keras.models import load_model\n",
    "# from gtts import gTTS\n",
    "# import os\n",
    "# import time\n",
    "\n",
    "# # Load your model\n",
    "# model = load_model('emotion_detection_model.h5')\n",
    "\n",
    "# # Function to predict emotion from an image\n",
    "# def predict_emotion(model, img):\n",
    "#     img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "#     img = cv2.resize(img, (48, 48))\n",
    "#     img = img.reshape(1, 48, 48, 1) / 255.0\n",
    "#     prediction = model.predict(img)\n",
    "#     emotion = np.argmax(prediction)\n",
    "#     return emotion\n",
    "\n",
    "# # Function to generate and play a personalized message\n",
    "# def play_message(emotion):\n",
    "#     emotion_dict = {\n",
    "#         0: 'Aaj aapka mood bahut accha lag raha hai. Lijiye aapke mood ko aur accha karne ke liye pesh hai ek pyara sa song.',\n",
    "#         1: 'Aaj aap thode udas lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa song.',\n",
    "#         2: 'Aap bahut chakit lag rahe hain! Lijiye aapke mood ko aur accha karne ke liye pesh hai ek pyara sa song.',\n",
    "#         3: 'Aap bilkul shaant lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "#         4: 'Aap thoda dar rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "#         5: 'Aapko kuch ghin aa rahi hai. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "#         6: 'Aap gusse mein lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.'\n",
    "#     }\n",
    "\n",
    "#     message = emotion_dict[emotion]\n",
    "\n",
    "    # # Ensure no other process is using the file and delete if it exists\n",
    "    # if os.path.exists(\"message.mp3\"):\n",
    "    #     try:\n",
    "    #         os.remove(\"message.mp3\")\n",
    "    #     except PermissionError:\n",
    "    #         print(\"Error: Unable to delete existing 'message.mp3' file.\")\n",
    "    #         return\n",
    "\n",
    "    # tts = gTTS(text=message, lang='hi')\n",
    "    # tts.save(\"message.mp3\")\n",
    "\n",
    "    # pygame.mixer.init()\n",
    "    # pygame.mixer.music.load(\"message.mp3\")\n",
    "#     pygame.mixer.music.play()\n",
    "#     while pygame.mixer.music.get_busy():\n",
    "#         continue\n",
    "\n",
    "# # Function to play a song based on the emotion\n",
    "# def play_song(emotion):\n",
    "#     emotion_dict = {0: 'happy', 1: 'sad', 2: 'surprise', 3: 'neutral', 4: 'fear', 5: 'disgust', 6: 'angry'}\n",
    "#     songs = {\n",
    "#         'happy': ['happy.mpeg', 'happy1.mpeg'],\n",
    "#         'sad': ['sad.mpeg', 'sad1.mpeg'],\n",
    "#         'surprise': ['surprise.mpeg', 'surprise1.mpeg'],\n",
    "#         'neutral': ['netural2.mpeg', 'netural3.mpeg'],\n",
    "#         'fear': ['fear.mpeg', 'fear1.mpeg'],\n",
    "#         'disgust': ['disgust.mpeg', 'disgust1.mpeg'],\n",
    "#         'angry': ['angry.mpeg', 'angry1.mpeg']\n",
    "#     }\n",
    "    \n",
    "#     emotion_label = emotion_dict[emotion]\n",
    "#     song = random.choice(songs[emotion_label])\n",
    "    \n",
    "#     # Play the personalized message\n",
    "#     play_message(emotion)\n",
    "    \n",
    "#     # Play the song\n",
    "#     pygame.mixer.music.load(song)\n",
    "#     pygame.mixer.music.play()\n",
    "\n",
    "# # Start video capture\n",
    "# cap = cv2.VideoCapture(0)\n",
    "\n",
    "# # Initialize timer and state\n",
    "# emotion_detected = False\n",
    "\n",
    "# while True:\n",
    "#     ret, frame = cap.read()\n",
    "    # if not ret:\n",
    "    #     break\n",
    "\n",
    "    # if not emotion_detected:\n",
    "    #     # Define the region of interest for face detection\n",
    "    #     face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    #     gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #     faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "        \n",
    "    #     for (x, y, w, h) in faces:\n",
    "    #         face = frame[y:y+h, x:x+w]\n",
    "    #         emotion = predict_emotion(model, face)\n",
    "            \n",
    "    #         # Draw a rectangle around the face and label it with the emotion\n",
    "    #         cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    #         emotion_dict = {0: 'Happy', 1: 'Sad', 2: 'Surprise', 3: 'Neutral', 4: 'Fear', 5: 'Disgust', 6: 'Angry'}\n",
    "    #         emotion_text = emotion_dict[emotion]\n",
    "    #         cv2.putText(frame, emotion_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "            \n",
    "    #         # Play the song for the detected emotion\n",
    "    #         play_song(emotion)\n",
    "    #         emotion_detected = True\n",
    "    \n",
    "    # # Display the resulting frame\n",
    "    # cv2.imshow('Emotion Detection', frame)\n",
    "    \n",
    "    # # Check for key press to recapture emotion\n",
    "    # key = cv2.waitKey(1) & 0xFF\n",
    "    # if key == ord('r'):  # Press 'r' to recapture the image and detect new emotion\n",
    "    #     emotion_detected = False\n",
    "    # elif key == ord('q'):  # Press 'q' to quit\n",
    "#         break\n",
    "\n",
    "# # Release the capture and destroy all OpenCV windows\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# # Remove the temporary message file\n",
    "# if os.path.exists(\"message.mp3\"):\n",
    "#     try:\n",
    "#         os.remove(\"message.mp3\")\n",
    "#     except PermissionError:\n",
    "#         print(\"Error: Unable to delete 'message.mp3' file.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\lenovo\\anaconda3\\lib\\site-packages (1.26.4)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.5.2 (SDL 2.28.3, Python 3.11.7)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 462ms/step\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "No file 'netural.mpeg' found in working directory 'c:\\Users\\lenovo\\OneDrive\\Desktop\\emotion detect'.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m\n\u001b[0;32m    102\u001b[0m         cv2\u001b[38;5;241m.\u001b[39mputText(frame, emotion_text, (x, y\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m10\u001b[39m), cv2\u001b[38;5;241m.\u001b[39mFONT_HERSHEY_SIMPLEX, \u001b[38;5;241m0.9\u001b[39m, (\u001b[38;5;241m255\u001b[39m, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m    104\u001b[0m         \u001b[38;5;66;03m# Play the song for the detected emotion\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m         play_song(emotion)\n\u001b[0;32m    106\u001b[0m         emotion_detected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    108\u001b[0m \u001b[38;5;66;03m# # Display the resulting frame\u001b[39;00m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;66;03m# if frame is not None:\u001b[39;00m\n\u001b[0;32m    110\u001b[0m \u001b[38;5;66;03m#     cv2.imshow('Emotion Detection', frame)\u001b[39;00m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;66;03m# else:\u001b[39;00m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m#     print(\"Error: Frame is None\")\u001b[39;00m\n\u001b[0;32m    113\u001b[0m  \u001b[38;5;66;03m# Display the resulting frame\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 73\u001b[0m, in \u001b[0;36mplay_song\u001b[1;34m(emotion)\u001b[0m\n\u001b[0;32m     70\u001b[0m play_message(emotion)\n\u001b[0;32m     72\u001b[0m \u001b[38;5;66;03m# Play the song\u001b[39;00m\n\u001b[1;32m---> 73\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mload(song)\n\u001b[0;32m     74\u001b[0m pygame\u001b[38;5;241m.\u001b[39mmixer\u001b[38;5;241m.\u001b[39mmusic\u001b[38;5;241m.\u001b[39mplay()\n",
      "\u001b[1;31merror\u001b[0m: No file 'netural.mpeg' found in working directory 'c:\\Users\\lenovo\\OneDrive\\Desktop\\emotion detect'."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import pygame\n",
    "import random\n",
    "from tensorflow.keras.models import load_model\n",
    "from gtts import gTTS\n",
    "import os\n",
    "import time\n",
    "\n",
    "# Load your model\n",
    "model = load_model('emotion_detection_model.h5')\n",
    "\n",
    "# Function to predict emotion from an image\n",
    "def predict_emotion(model, img):\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    img = cv2.resize(img, (48, 48))\n",
    "    img = img.reshape(1, 48, 48, 1) / 255.0\n",
    "    prediction = model.predict(img)\n",
    "    emotion = np.argmax(prediction)\n",
    "    return emotion\n",
    "\n",
    "# Function to generate and play a personalized message\n",
    "def play_message(emotion):\n",
    "    emotion_dict = {\n",
    "        0: 'Aaj aapka mood bahut accha lag raha hai. Lijiye aapke mood ko aur accha karne ke liye pesh hai ek pyara sa song.',\n",
    "        1: 'Aaj aap thode udas lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa song.',\n",
    "        2: 'Aap bahut chakit lag rahe hain! Lijiye aapke mood ko aur accha karne ke liye pesh hai ek pyara sa song.',\n",
    "        3: 'Aap bilkul shaant lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "        4: 'Aap thoda dar rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "        5: 'Aapko kuch ghin aa rahi hai. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.',\n",
    "        6: 'Aap gusse mein lag rahe hain. Lijiye aapke mood ko behtar karne ke liye pesh hai ek pyara sa geet.'\n",
    "    }\n",
    "\n",
    "    message = emotion_dict[emotion]\n",
    "\n",
    "    # Ensure no other process is using the file and delete if it exists\n",
    "    if os.path.exists(\"message.mp3\"):\n",
    "        try:\n",
    "            os.remove(\"message.mp3\")\n",
    "        except PermissionError:\n",
    "            print(\"Error: Unable to delete existing 'message.mp3' file.\")\n",
    "            return\n",
    "\n",
    "    tts = gTTS(text=message, lang='hi')\n",
    "    tts.save(\"message.mp3\")\n",
    "\n",
    "    pygame.mixer.init()\n",
    "    pygame.mixer.music.load(\"message.mp3\")\n",
    "    pygame.mixer.music.play()\n",
    "    while pygame.mixer.music.get_busy():\n",
    "        continue\n",
    "\n",
    "# Function to play a song based on the emotion\n",
    "def play_song(emotion):\n",
    "    emotion_dict = {0: 'happy', 1: 'sad', 2: 'surprise', 3: 'neutral', 4: 'fear', 5: 'disgust', 6: 'angry'}\n",
    "    songs = {\n",
    "        'happy': ['happy.mpeg'],\n",
    "        'sad': ['sad.mpeg'],\n",
    "        'surprise': ['surprise.mpeg'],\n",
    "        'neutral': ['neutral.mpeg'],\n",
    "        'fear': ['Fear.mpeg'],\n",
    "        'disgust': ['Disguise.mpeg'],\n",
    "        'angry': ['angry.mpeg']\n",
    "    }\n",
    "\n",
    "    emotion_label = emotion_dict[emotion]\n",
    "    song = random.choice(songs[emotion_label])\n",
    "\n",
    "    # Play the personalized message\n",
    "    play_message(emotion)\n",
    "\n",
    "    # Play the song\n",
    "    pygame.mixer.music.load(song)\n",
    "    pygame.mixer.music.play()\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Initialize timer and state\n",
    "emotion_detected = False\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        print(\"Error: Unable to capture video\")\n",
    "        break\n",
    "\n",
    "    if not emotion_detected:\n",
    "        # Define the region of interest for face detection\n",
    "        face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "        faces = face_cascade.detectMultiScale(gray_frame, scaleFactor=1.1, minNeighbors=5)\n",
    "\n",
    "        for (x, y, w, h) in faces:\n",
    "            face = frame[y:y+h, x:x+w]\n",
    "            emotion = predict_emotion(model, face)\n",
    "\n",
    "            # Draw a rectangle around the face and label it with the emotion\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "            emotion_dict = {0: 'Happy', 1: 'Sad', 2: 'Surprise', 3: 'Neutral', 4: 'Fear', 5: 'Disgust', 6: 'Angry'}\n",
    "            emotion_text = emotion_dict[emotion]\n",
    "            cv2.putText(frame, emotion_text, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "            # Play the song for the detected emotion\n",
    "            play_song(emotion)\n",
    "            emotion_detected = True\n",
    "\n",
    "    # # Display the resulting frame\n",
    "    # if frame is not None:\n",
    "    #     cv2.imshow('Emotion Detection', frame)\n",
    "    # else:\n",
    "    #     print(\"Error: Frame is None\")\n",
    "     # Display the resulting frame\n",
    "    cv2.imshow('Emotion Detection', frame)\n",
    "\n",
    "    # Check for key press to recapture emotion\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord('r'):  # Press 'r' to recapture the image and detect new emotion\n",
    "        emotion_detected = False\n",
    "    elif key == ord('q'):  # Press 'q' to quit\n",
    "        break\n",
    "\n",
    "# Release the capture and destroy all OpenCV windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "# Remove the temporary message file\n",
    "if os.path.exists(\"message.mp3\"):\n",
    "    try:\n",
    "        os.remove(\"message.mp3\")\n",
    "    except PermissionError:\n",
    "        print(\"Error: Unable to delete 'message.mp3' file.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
